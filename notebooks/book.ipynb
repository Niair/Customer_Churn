{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gUSv1YBAeOh"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "x31MKP9qGwOA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "oA5DCl6pHQcp"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "fAHU8XsrAjqw",
        "outputId": "2365bfcc-c210-455d-9eff-18c4f511965d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>married</th>\n",
              "      <th>number_of_dependents</th>\n",
              "      <th>city</th>\n",
              "      <th>zip_code</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>number_of_referrals</th>\n",
              "      <th>tenure_in_months</th>\n",
              "      <th>offer</th>\n",
              "      <th>phone_service</th>\n",
              "      <th>avg_monthly_long_distance_charges</th>\n",
              "      <th>multiple_lines</th>\n",
              "      <th>internet_service</th>\n",
              "      <th>internet_type</th>\n",
              "      <th>avg_monthly_gb_download</th>\n",
              "      <th>online_security</th>\n",
              "      <th>online_backup</th>\n",
              "      <th>device_protection_plan</th>\n",
              "      <th>premium_tech_support</th>\n",
              "      <th>streaming_tv</th>\n",
              "      <th>streaming_movies</th>\n",
              "      <th>streaming_music</th>\n",
              "      <th>unlimited_data</th>\n",
              "      <th>contract</th>\n",
              "      <th>paperless_billing</th>\n",
              "      <th>payment_method</th>\n",
              "      <th>monthly_charge</th>\n",
              "      <th>total_charges</th>\n",
              "      <th>total_refunds</th>\n",
              "      <th>total_extra_data_charges</th>\n",
              "      <th>total_long_distance_charges</th>\n",
              "      <th>total_revenue</th>\n",
              "      <th>customer_status</th>\n",
              "      <th>has_offer</th>\n",
              "      <th>offer_popularity</th>\n",
              "      <th>has_extra_internet_charges</th>\n",
              "      <th>tenure_category</th>\n",
              "      <th>engagement_score</th>\n",
              "      <th>high_value</th>\n",
              "      <th>num_addon_services</th>\n",
              "      <th>churned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6878</th>\n",
              "      <td>9770-LXDBK</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>90006</td>\n",
              "      <td>34.048013</td>\n",
              "      <td>-118.293953</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Offer E</td>\n",
              "      <td>Yes</td>\n",
              "      <td>14</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No Internet</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Month-to-Month</td>\n",
              "      <td>No</td>\n",
              "      <td>Credit Card</td>\n",
              "      <td>20.40</td>\n",
              "      <td>63.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>44.37</td>\n",
              "      <td>107.52</td>\n",
              "      <td>Joined</td>\n",
              "      <td>1</td>\n",
              "      <td>0.114298</td>\n",
              "      <td>0</td>\n",
              "      <td>0-1 Year</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6705</th>\n",
              "      <td>9522-ZSINC</td>\n",
              "      <td>Male</td>\n",
              "      <td>34</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>Santa Rosa</td>\n",
              "      <td>95409</td>\n",
              "      <td>38.468893</td>\n",
              "      <td>-122.580539</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>Offer D</td>\n",
              "      <td>Yes</td>\n",
              "      <td>7</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No Internet</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>One Year</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Bank Withdrawal</td>\n",
              "      <td>19.95</td>\n",
              "      <td>253.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>95.29</td>\n",
              "      <td>349.09</td>\n",
              "      <td>Stayed</td>\n",
              "      <td>1</td>\n",
              "      <td>0.085475</td>\n",
              "      <td>0</td>\n",
              "      <td>1-2 Years</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2941</th>\n",
              "      <td>4193-ORFCL</td>\n",
              "      <td>Female</td>\n",
              "      <td>80</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>North Hollywood</td>\n",
              "      <td>91606</td>\n",
              "      <td>34.187599</td>\n",
              "      <td>-118.387125</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Offer E</td>\n",
              "      <td>Yes</td>\n",
              "      <td>15</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>DSL</td>\n",
              "      <td>14.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Month-to-Month</td>\n",
              "      <td>No</td>\n",
              "      <td>Credit Card</td>\n",
              "      <td>45.10</td>\n",
              "      <td>45.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>15.24</td>\n",
              "      <td>60.34</td>\n",
              "      <td>Churned</td>\n",
              "      <td>1</td>\n",
              "      <td>0.114298</td>\n",
              "      <td>0</td>\n",
              "      <td>0-1 Year</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4297</th>\n",
              "      <td>6050-IJRHS</td>\n",
              "      <td>Female</td>\n",
              "      <td>47</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "      <td>Chino Hills</td>\n",
              "      <td>91709</td>\n",
              "      <td>33.942895</td>\n",
              "      <td>-117.725644</td>\n",
              "      <td>2</td>\n",
              "      <td>70</td>\n",
              "      <td>Offer A</td>\n",
              "      <td>Yes</td>\n",
              "      <td>5</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Fiber Optic</td>\n",
              "      <td>19.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>One Year</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Bank Withdrawal</td>\n",
              "      <td>106.50</td>\n",
              "      <td>7397.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>415.10</td>\n",
              "      <td>7812.10</td>\n",
              "      <td>Stayed</td>\n",
              "      <td>1</td>\n",
              "      <td>0.073832</td>\n",
              "      <td>0</td>\n",
              "      <td>4+ Years</td>\n",
              "      <td>7</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3508</th>\n",
              "      <td>4973-MGTON</td>\n",
              "      <td>Female</td>\n",
              "      <td>33</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>Topaz</td>\n",
              "      <td>96133</td>\n",
              "      <td>38.636052</td>\n",
              "      <td>-119.489162</td>\n",
              "      <td>0</td>\n",
              "      <td>71</td>\n",
              "      <td>No Offer</td>\n",
              "      <td>Yes</td>\n",
              "      <td>3</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>DSL</td>\n",
              "      <td>26.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Two Year</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Credit Card</td>\n",
              "      <td>84.40</td>\n",
              "      <td>5969.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>219.39</td>\n",
              "      <td>6188.69</td>\n",
              "      <td>Stayed</td>\n",
              "      <td>0</td>\n",
              "      <td>0.550476</td>\n",
              "      <td>0</td>\n",
              "      <td>4+ Years</td>\n",
              "      <td>8</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6652</th>\n",
              "      <td>9465-RWMXL</td>\n",
              "      <td>Male</td>\n",
              "      <td>62</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0</td>\n",
              "      <td>San Diego</td>\n",
              "      <td>92126</td>\n",
              "      <td>32.886925</td>\n",
              "      <td>-117.152162</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>Offer C</td>\n",
              "      <td>Yes</td>\n",
              "      <td>22</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Fiber Optic</td>\n",
              "      <td>11.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Month-to-Month</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Bank Withdrawal</td>\n",
              "      <td>78.90</td>\n",
              "      <td>2447.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>722.24</td>\n",
              "      <td>3170.19</td>\n",
              "      <td>Churned</td>\n",
              "      <td>1</td>\n",
              "      <td>0.058924</td>\n",
              "      <td>0</td>\n",
              "      <td>2-4 Years</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     customer_id  gender  age married  number_of_dependents             city  \\\n",
              "6878  9770-LXDBK  Female   42      No                     0      Los Angeles   \n",
              "6705  9522-ZSINC    Male   34      No                     0       Santa Rosa   \n",
              "2941  4193-ORFCL  Female   80      No                     0  North Hollywood   \n",
              "4297  6050-IJRHS  Female   47     Yes                     1      Chino Hills   \n",
              "3508  4973-MGTON  Female   33     Yes                     0            Topaz   \n",
              "6652  9465-RWMXL    Male   62     Yes                     0        San Diego   \n",
              "\n",
              "      zip_code   latitude   longitude  number_of_referrals  tenure_in_months  \\\n",
              "6878     90006  34.048013 -118.293953                    0                 3   \n",
              "6705     95409  38.468893 -122.580539                    0                13   \n",
              "2941     91606  34.187599 -118.387125                    0                 1   \n",
              "4297     91709  33.942895 -117.725644                    2                70   \n",
              "3508     96133  38.636052 -119.489162                    0                71   \n",
              "6652     92126  32.886925 -117.152162                    1                32   \n",
              "\n",
              "         offer phone_service  avg_monthly_long_distance_charges  \\\n",
              "6878   Offer E           Yes                                 14   \n",
              "6705   Offer D           Yes                                  7   \n",
              "2941   Offer E           Yes                                 15   \n",
              "4297   Offer A           Yes                                  5   \n",
              "3508  No Offer           Yes                                  3   \n",
              "6652   Offer C           Yes                                 22   \n",
              "\n",
              "     multiple_lines internet_service internet_type  avg_monthly_gb_download  \\\n",
              "6878             No               No   No Internet                      0.0   \n",
              "6705             No               No   No Internet                      0.0   \n",
              "2941             No              Yes           DSL                     14.0   \n",
              "4297            Yes              Yes   Fiber Optic                     19.0   \n",
              "3508             No              Yes           DSL                     26.0   \n",
              "6652            Yes              Yes   Fiber Optic                     11.0   \n",
              "\n",
              "     online_security online_backup device_protection_plan  \\\n",
              "6878              No            No                     No   \n",
              "6705              No            No                     No   \n",
              "2941              No            No                     No   \n",
              "4297              No           Yes                    Yes   \n",
              "3508             Yes           Yes                    Yes   \n",
              "6652              No            No                    Yes   \n",
              "\n",
              "     premium_tech_support streaming_tv streaming_movies streaming_music  \\\n",
              "6878                   No           No               No              No   \n",
              "6705                   No           No               No              No   \n",
              "2941                   No           No               No              No   \n",
              "4297                   No          Yes              Yes             Yes   \n",
              "3508                  Yes          Yes              Yes             Yes   \n",
              "6652                   No           No               No              No   \n",
              "\n",
              "     unlimited_data        contract paperless_billing   payment_method  \\\n",
              "6878             No  Month-to-Month                No      Credit Card   \n",
              "6705             No        One Year               Yes  Bank Withdrawal   \n",
              "2941            Yes  Month-to-Month                No      Credit Card   \n",
              "4297            Yes        One Year               Yes  Bank Withdrawal   \n",
              "3508            Yes        Two Year               Yes      Credit Card   \n",
              "6652            Yes  Month-to-Month               Yes  Bank Withdrawal   \n",
              "\n",
              "      monthly_charge  total_charges  total_refunds  total_extra_data_charges  \\\n",
              "6878           20.40          63.15            0.0                         0   \n",
              "6705           19.95         253.80            0.0                         0   \n",
              "2941           45.10          45.10            0.0                         0   \n",
              "4297          106.50        7397.00            0.0                         0   \n",
              "3508           84.40        5969.30            0.0                         0   \n",
              "6652           78.90        2447.95            0.0                         0   \n",
              "\n",
              "      total_long_distance_charges  total_revenue customer_status  has_offer  \\\n",
              "6878                        44.37         107.52          Joined          1   \n",
              "6705                        95.29         349.09          Stayed          1   \n",
              "2941                        15.24          60.34         Churned          1   \n",
              "4297                       415.10        7812.10          Stayed          1   \n",
              "3508                       219.39        6188.69          Stayed          0   \n",
              "6652                       722.24        3170.19         Churned          1   \n",
              "\n",
              "      offer_popularity  has_extra_internet_charges tenure_category  \\\n",
              "6878          0.114298                           0        0-1 Year   \n",
              "6705          0.085475                           0       1-2 Years   \n",
              "2941          0.114298                           0        0-1 Year   \n",
              "4297          0.073832                           0        4+ Years   \n",
              "3508          0.550476                           0        4+ Years   \n",
              "6652          0.058924                           0       2-4 Years   \n",
              "\n",
              "      engagement_score  high_value  num_addon_services  churned  \n",
              "6878                 0       False                   0    False  \n",
              "6705                 0       False                   0    False  \n",
              "2941                 1       False                   0     True  \n",
              "4297                 7        True                   2    False  \n",
              "3508                 8        True                   4    False  \n",
              "6652                 3       False                   1     True  "
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"E:/VS_Code/Projects/Customer_Churn/notebooks/data/processed_customer_churn_data.csv\")\n",
        "\n",
        "df.sample(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7043, 44)"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdIXTgnO6VxS"
      },
      "source": [
        "#### **MODEL with Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BnvkLcoczO5v",
        "outputId": "887a6a6b-46c0-484c-bb2f-0b85e5d0c46d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: category_encoders in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from category_encoders) (2.3.1)\n",
            "Requirement already satisfied: pandas>=1.0.5 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from category_encoders) (2.3.1)\n",
            "Requirement already satisfied: patsy>=0.5.1 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from category_encoders) (1.7.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from category_encoders) (1.16.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from category_encoders) (0.14.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "nWsXp3gjzO3T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp313-cp313-win_amd64.whl.metadata (1.5 kB)\n",
            "Collecting graphviz (from catboost)\n",
            "  Using cached graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from catboost) (3.10.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from catboost) (2.3.1)\n",
            "Requirement already satisfied: pandas>=0.24 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from catboost) (2.3.1)\n",
            "Requirement already satisfied: scipy in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: plotly in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from catboost) (6.2.0)\n",
            "Requirement already satisfied: six in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from matplotlib->catboost) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from plotly->catboost) (1.46.0)\n",
            "Downloading catboost-1.2.8-cp313-cp313-win_amd64.whl (102.4 MB)\n",
            "   ---------------------------------------- 0.0/102.4 MB ? eta -:--:--\n",
            "   - -------------------------------------- 4.7/102.4 MB 22.4 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 9.7/102.4 MB 25.6 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 19.4/102.4 MB 31.0 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 35.7/102.4 MB 43.9 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 35.7/102.4 MB 43.9 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 35.7/102.4 MB 43.9 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 35.7/102.4 MB 43.9 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 35.9/102.4 MB 21.0 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 55.8/102.4 MB 29.0 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 73.1/102.4 MB 34.5 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 78.6/102.4 MB 33.8 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 83.6/102.4 MB 33.1 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 89.9/102.4 MB 32.8 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 94.6/102.4 MB 32.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  100.1/102.4 MB 31.6 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.2/102.4 MB 31.4 MB/s eta 0:00:01\n",
            "   --------------------------------------- 102.4/102.4 MB 11.0 MB/s eta 0:00:00\n",
            "Using cached graphviz-0.21-py3-none-any.whl (47 kB)\n",
            "Installing collected packages: graphviz, catboost\n",
            "\n",
            "   ---------------------------------------- 0/2 [graphviz]\n",
            "   ---------------------------------------- 0/2 [graphviz]\n",
            "   ---------------------------------------- 0/2 [graphviz]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   -------------------- ------------------- 1/2 [catboost]\n",
            "   ---------------------------------------- 2/2 [catboost]\n",
            "\n",
            "Successfully installed catboost-1.2.8 graphviz-0.21\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5bZkrkp-9QLu",
        "outputId": "db6df4cb-0870-4e17-873a-847491bdbac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from optuna) (2.3.1)\n",
            "Requirement already satisfied: packaging>=20.0 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from optuna) (25.0)\n",
            "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
            "  Using cached sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
            "Collecting tqdm (from optuna)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: PyYAML in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
            "  Downloading greenlet-3.2.3-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: colorama in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in e:\\vs_code\\projects\\customer_churn\\venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "Downloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "Using cached sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl (2.1 MB)\n",
            "Downloading greenlet-3.2.3-cp313-cp313-win_amd64.whl (297 kB)\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
            "\n",
            "   ---------------------------------------- 0/7 [tqdm]\n",
            "   ---------------------------------------- 0/7 [tqdm]\n",
            "   ---------------------------------------- 0/7 [tqdm]\n",
            "   ---------------------------------------- 0/7 [tqdm]\n",
            "   ---------------------------------------- 0/7 [tqdm]\n",
            "   ----- ---------------------------------- 1/7 [Mako]\n",
            "   ----- ---------------------------------- 1/7 [Mako]\n",
            "   ----- ---------------------------------- 1/7 [Mako]\n",
            "   ----- ---------------------------------- 1/7 [Mako]\n",
            "   ----------- ---------------------------- 2/7 [greenlet]\n",
            "   ----------- ---------------------------- 2/7 [greenlet]\n",
            "   ----------- ---------------------------- 2/7 [greenlet]\n",
            "   ----------------- ---------------------- 3/7 [colorlog]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------- ----------------- 4/7 [sqlalchemy]\n",
            "   ---------------------------- ----------- 5/7 [alembic]\n",
            "   ---------------------------- ----------- 5/7 [alembic]\n",
            "   ---------------------------- ----------- 5/7 [alembic]\n",
            "   ---------------------------- ----------- 5/7 [alembic]\n",
            "   ---------------------------- ----------- 5/7 [alembic]\n",
            "   ---------------------------- ----------- 5/7 [alembic]\n",
            "   ---------------------------- ----------- 5/7 [alembic]\n",
            "   ---------------------------- ----------- 5/7 [alembic]\n",
            "   ---------------------------- ----------- 5/7 [alembic]\n",
            "   ---------------------------- ----------- 5/7 [alembic]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------- ----- 6/7 [optuna]\n",
            "   ---------------------------------------- 7/7 [optuna]\n",
            "\n",
            "Successfully installed Mako-1.3.10 alembic-1.16.4 colorlog-6.9.0 greenlet-3.2.3 optuna-4.4.0 sqlalchemy-2.0.41 tqdm-4.67.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k_GMwfiNITtC",
        "outputId": "0f7b7e65-214d-48f0-8068-2a8983bf3174"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from category_encoders import CatBoostEncoder\n",
        "from category_encoders.target_encoder import TargetEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Cl5IDPzrIIbK"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, cross_validate\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, accuracy_score, precision_score, \n",
        "    recall_score, f1_score, confusion_matrix, classification_report\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.visualization import plot_param_importances, plot_optimization_history\n",
        "from optuna.exceptions import TrialPruned\n",
        "\n",
        "import joblib\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from src.logger import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-07-15 21:07:38,178] 88 root - INFO - Encoded categorical target variable\n",
            "[2025-07-15 21:07:38,179] 93 root - INFO - Multiclass classification task detected\n",
            "[2025-07-15 21:07:38,185] 79 root - INFO - Data prepared. Train: (5634, 43), Test: (1409, 43)\n",
            "[2025-07-15 21:07:38,186] 80 root - INFO - Numerical features: 20, Categorical features: 23\n",
            "[2025-07-15 21:07:38,187] 198 root - INFO - Components configured: 3 encoders, 3 scalers, 3 feature selectors, 5 models\n",
            "[2025-07-15 21:07:38,187] 59 root - INFO - AutoML Trainer initialized successfully\n",
            "[2025-07-15 21:07:38,188] 329 root - INFO - \n",
            "==================================================\n",
            "[2025-07-15 21:07:38,188] 330 root - INFO - Optimizing XGBoost_target_standard_variance\n",
            "[2025-07-15 21:07:38,188] 331 root - INFO - ==================================================\n",
            "[I 2025-07-15 21:07:38,189] A new study created in memory with name: no-name-36c76d9a-99e5-48ae-8d62-b9e93d44bf9a\n",
            "[2025-07-15 21:07:38,193] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,194] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,195] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,196] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,196] Trial 0 finished with value: 0.5 and parameters: {'n_estimators': 250, 'learning_rate': 0.2536999076681772, 'max_depth': 7, 'subsample': 0.8795975452591109}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,199] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,200] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,201] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,202] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,202] Trial 1 finished with value: 0.5 and parameters: {'n_estimators': 162, 'learning_rate': 0.01699897838270077, 'max_depth': 3, 'subsample': 0.9598528437324805}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,206] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,207] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,208] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,209] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,209] Trial 2 finished with value: 0.5 and parameters: {'n_estimators': 341, 'learning_rate': 0.11114989443094977, 'max_depth': 3, 'subsample': 0.9909729556485982}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,212] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,213] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,214] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,215] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,216] Trial 3 finished with value: 0.5 and parameters: {'n_estimators': 433, 'learning_rate': 0.020589728197687916, 'max_depth': 4, 'subsample': 0.7550213529560301}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,219] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,220] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,221] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,222] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,222] Trial 4 finished with value: 0.5 and parameters: {'n_estimators': 222, 'learning_rate': 0.05958389350068958, 'max_depth': 5, 'subsample': 0.7873687420594125}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,225] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,226] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,227] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,228] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,228] Trial 5 finished with value: 0.5 and parameters: {'n_estimators': 345, 'learning_rate': 0.01607123851203988, 'max_depth': 4, 'subsample': 0.8099085529881075}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,231] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,232] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,233] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,234] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,234] Trial 6 finished with value: 0.5 and parameters: {'n_estimators': 282, 'learning_rate': 0.14447746112718687, 'max_depth': 4, 'subsample': 0.8542703315240835}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,237] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,238] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,239] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,240] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,240] Trial 7 finished with value: 0.5 and parameters: {'n_estimators': 337, 'learning_rate': 0.011711509955524094, 'max_depth': 6, 'subsample': 0.7511572371061874}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,243] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,244] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,245] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,245] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,246] Trial 8 finished with value: 0.5 and parameters: {'n_estimators': 126, 'learning_rate': 0.2521267904777921, 'max_depth': 8, 'subsample': 0.9425192044349383}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,248] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,250] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,252] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,252] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,253] Trial 9 finished with value: 0.5 and parameters: {'n_estimators': 222, 'learning_rate': 0.013940346079873234, 'max_depth': 7, 'subsample': 0.8320457481218804}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,263] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,264] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,266] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,266] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,267] Trial 10 finished with value: 0.5 and parameters: {'n_estimators': 478, 'learning_rate': 0.0500220915772748, 'max_depth': 8, 'subsample': 0.8919394579380205}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,277] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,279] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,280] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,280] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,281] Trial 11 finished with value: 0.5 and parameters: {'n_estimators': 115, 'learning_rate': 0.032624228995528144, 'max_depth': 6, 'subsample': 0.9216947866421255}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,292] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,293] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,295] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,295] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,295] Trial 12 finished with value: 0.5 and parameters: {'n_estimators': 186, 'learning_rate': 0.2818786515020195, 'max_depth': 7, 'subsample': 0.9854945337124003}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,307] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,309] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,310] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,310] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,311] Trial 13 finished with value: 0.5 and parameters: {'n_estimators': 177, 'learning_rate': 0.027365736357468685, 'max_depth': 3, 'subsample': 0.8840006794174039}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,321] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,323] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,324] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,325] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,326] Trial 14 finished with value: 0.5 and parameters: {'n_estimators': 271, 'learning_rate': 0.09148397126162978, 'max_depth': 7, 'subsample': 0.9406212447331025}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,336] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,337] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,339] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,339] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,339] Trial 15 finished with value: 0.5 and parameters: {'n_estimators': 163, 'learning_rate': 0.17119248177483057, 'max_depth': 5, 'subsample': 0.8863312792477095}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,351] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,352] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,354] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,354] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,355] Trial 16 finished with value: 0.5 and parameters: {'n_estimators': 239, 'learning_rate': 0.054403312289292724, 'max_depth': 6, 'subsample': 0.9691761017788351}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,366] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,368] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,369] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,369] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,370] Trial 17 finished with value: 0.5 and parameters: {'n_estimators': 151, 'learning_rate': 0.032770075817163305, 'max_depth': 5, 'subsample': 0.9259412902730204}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,380] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,381] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,383] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,383] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,384] Trial 18 finished with value: 0.5 and parameters: {'n_estimators': 299, 'learning_rate': 0.09548424890644631, 'max_depth': 7, 'subsample': 0.8416629028975215}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,395] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,396] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,397] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,398] 301 root - INFO - XGBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,398] Trial 19 finished with value: 0.5 and parameters: {'n_estimators': 392, 'learning_rate': 0.01046111763034609, 'max_depth': 3, 'subsample': 0.9597550967973407}. Best is trial 0 with value: 0.5.\n",
            "e:\\VS_Code\\Projects\\Customer_Churn\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "[2025-07-15 21:07:38,753] 396 root - INFO - XGBoost_target_standard_variance | Test AUC: 0.7933\n",
            "[2025-07-15 21:07:38,754] 430 root - ERROR - Optimization failed for XGBoost_target_standard_variance: 'ColumnTransformer' object has no attribute 'named_transformers'\n",
            "[2025-07-15 21:07:38,755] 329 root - INFO - \n",
            "==================================================\n",
            "[2025-07-15 21:07:38,755] 330 root - INFO - Optimizing RandomForest_onehot_minmax_mutual_info\n",
            "[2025-07-15 21:07:38,756] 331 root - INFO - ==================================================\n",
            "[I 2025-07-15 21:07:38,757] A new study created in memory with name: no-name-6b7d97d8-97cf-4efc-93be-9260c014d9d7\n",
            "[2025-07-15 21:07:38,760] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,762] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,763] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,763] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,764] Trial 0 finished with value: 0.5 and parameters: {'n_estimators': 250, 'max_depth': 20, 'min_samples_split': 12}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,767] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,769] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,770] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,771] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,771] Trial 1 finished with value: 0.5 and parameters: {'n_estimators': 340, 'max_depth': 7, 'min_samples_split': 4}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,774] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,776] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,777] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,778] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,778] Trial 2 finished with value: 0.5 and parameters: {'n_estimators': 123, 'max_depth': 18, 'min_samples_split': 10}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,781] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,782] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,784] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,784] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,785] Trial 3 finished with value: 0.5 and parameters: {'n_estimators': 383, 'max_depth': 5, 'min_samples_split': 15}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,788] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,790] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,791] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,792] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,792] Trial 4 finished with value: 0.5 and parameters: {'n_estimators': 433, 'max_depth': 8, 'min_samples_split': 4}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,795] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,797] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,798] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,799] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,800] Trial 5 finished with value: 0.5 and parameters: {'n_estimators': 173, 'max_depth': 9, 'min_samples_split': 9}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,803] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,805] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,807] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,807] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,808] Trial 6 finished with value: 0.5 and parameters: {'n_estimators': 273, 'max_depth': 9, 'min_samples_split': 10}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,812] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,814] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,815] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,816] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,817] Trial 7 finished with value: 0.5 and parameters: {'n_estimators': 155, 'max_depth': 9, 'min_samples_split': 7}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,820] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,822] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,824] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,824] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,825] Trial 8 finished with value: 0.5 and parameters: {'n_estimators': 282, 'max_depth': 17, 'min_samples_split': 4}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,828] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,830] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,832] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,832] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,833] Trial 9 finished with value: 0.5 and parameters: {'n_estimators': 306, 'max_depth': 14, 'min_samples_split': 2}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,846] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,847] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,849] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,849] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,850] Trial 10 finished with value: 0.5 and parameters: {'n_estimators': 224, 'max_depth': 15, 'min_samples_split': 14}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,859] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,861] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,862] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,862] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,863] Trial 11 finished with value: 0.5 and parameters: {'n_estimators': 356, 'max_depth': 20, 'min_samples_split': 12}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,872] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,874] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,875] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,875] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,876] Trial 12 finished with value: 0.5 and parameters: {'n_estimators': 475, 'max_depth': 5, 'min_samples_split': 7}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,885] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,886] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,888] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,888] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,889] Trial 13 finished with value: 0.5 and parameters: {'n_estimators': 340, 'max_depth': 11, 'min_samples_split': 12}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,898] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,899] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,901] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,902] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,902] Trial 14 finished with value: 0.5 and parameters: {'n_estimators': 226, 'max_depth': 12, 'min_samples_split': 6}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,913] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,914] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,916] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,917] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,917] Trial 15 finished with value: 0.5 and parameters: {'n_estimators': 402, 'max_depth': 20, 'min_samples_split': 12}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,928] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,930] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,931] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,932] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,933] Trial 16 finished with value: 0.5 and parameters: {'n_estimators': 229, 'max_depth': 7, 'min_samples_split': 3}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,944] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,945] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,947] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,947] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,948] Trial 17 finished with value: 0.5 and parameters: {'n_estimators': 314, 'max_depth': 14, 'min_samples_split': 5}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,958] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,959] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,961] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,961] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,962] Trial 18 finished with value: 0.5 and parameters: {'n_estimators': 481, 'max_depth': 17, 'min_samples_split': 8}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:07:38,971] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,973] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,974] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:07:38,975] 301 root - INFO - RandomForest Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:07:38,975] Trial 19 finished with value: 0.5 and parameters: {'n_estimators': 254, 'max_depth': 7, 'min_samples_split': 13}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,103] 396 root - INFO - RandomForest_onehot_minmax_mutual_info | Test AUC: 0.9975\n",
            "[2025-07-15 21:09:15,185] 430 root - ERROR - Optimization failed for RandomForest_onehot_minmax_mutual_info: 'ColumnTransformer' object has no attribute 'named_transformers'\n",
            "[2025-07-15 21:09:15,185] 329 root - INFO - \n",
            "==================================================\n",
            "[2025-07-15 21:09:15,186] 330 root - INFO - Optimizing CatBoost_catboost_none_none\n",
            "[2025-07-15 21:09:15,186] 331 root - INFO - ==================================================\n",
            "[I 2025-07-15 21:09:15,187] A new study created in memory with name: no-name-c0f516c3-780f-4fed-8bd7-2242a00ffd51\n",
            "[2025-07-15 21:09:15,190] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,192] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,193] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,194] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,194] Trial 0 finished with value: 0.5 and parameters: {'iterations': 687, 'learning_rate': 0.2536999076681772, 'depth': 7, 'l2_leaf_reg': 3.3946339367881464}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,197] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,199] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,201] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,202] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,202] Trial 1 finished with value: 0.5 and parameters: {'iterations': 578, 'learning_rate': 0.01699897838270077, 'depth': 4, 'l2_leaf_reg': 4.46470458309974}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,205] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,206] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,207] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,208] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,208] Trial 2 finished with value: 0.5 and parameters: {'iterations': 801, 'learning_rate': 0.11114989443094977, 'depth': 4, 'l2_leaf_reg': 4.879639408647977}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,212] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,214] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,215] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,216] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,216] Trial 3 finished with value: 0.5 and parameters: {'iterations': 917, 'learning_rate': 0.020589728197687916, 'depth': 4, 'l2_leaf_reg': 1.7336180394137353}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,219] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,220] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,222] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,222] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,223] Trial 4 finished with value: 0.5 and parameters: {'iterations': 652, 'learning_rate': 0.05958389350068958, 'depth': 6, 'l2_leaf_reg': 2.1649165607921677}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,225] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,227] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,228] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,229] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,229] Trial 5 finished with value: 0.5 and parameters: {'iterations': 806, 'learning_rate': 0.01607123851203988, 'depth': 5, 'l2_leaf_reg': 2.465447373174767}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,232] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,234] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,235] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,236] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,236] Trial 6 finished with value: 0.5 and parameters: {'iterations': 728, 'learning_rate': 0.14447746112718687, 'depth': 4, 'l2_leaf_reg': 3.0569377536544464}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,239] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,240] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,242] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,242] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,242] Trial 7 finished with value: 0.5 and parameters: {'iterations': 796, 'learning_rate': 0.011711509955524094, 'depth': 7, 'l2_leaf_reg': 1.6820964947491661}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,246] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,248] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,249] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,250] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,251] Trial 8 finished with value: 0.5 and parameters: {'iterations': 532, 'learning_rate': 0.2521267904777921, 'depth': 8, 'l2_leaf_reg': 4.233589392465845}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,253] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,254] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,256] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,256] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,257] Trial 9 finished with value: 0.5 and parameters: {'iterations': 652, 'learning_rate': 0.013940346079873234, 'depth': 7, 'l2_leaf_reg': 2.760609974958405}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,268] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,269] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,271] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,272] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,272] Trial 10 finished with value: 0.5 and parameters: {'iterations': 973, 'learning_rate': 0.0500220915772748, 'depth': 8, 'l2_leaf_reg': 3.55919277250694}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,284] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,285] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,287] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,287] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,288] Trial 11 finished with value: 0.5 and parameters: {'iterations': 519, 'learning_rate': 0.032624228995528144, 'depth': 6, 'l2_leaf_reg': 3.9559304885616733}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,299] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,301] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,302] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,303] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,304] Trial 12 finished with value: 0.5 and parameters: {'iterations': 608, 'learning_rate': 0.2818786515020195, 'depth': 5, 'l2_leaf_reg': 4.806593782832004}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,315] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,317] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,318] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,319] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,319] Trial 13 finished with value: 0.5 and parameters: {'iterations': 596, 'learning_rate': 0.027365736357468685, 'depth': 7, 'l2_leaf_reg': 3.4533423922320523}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,330] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,332] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,333] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,334] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,334] Trial 14 finished with value: 0.5 and parameters: {'iterations': 713, 'learning_rate': 0.09148397126162978, 'depth': 5, 'l2_leaf_reg': 4.208283263108034}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,345] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,347] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,348] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,349] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,349] Trial 15 finished with value: 0.5 and parameters: {'iterations': 579, 'learning_rate': 0.17119248177483057, 'depth': 6, 'l2_leaf_reg': 3.4844170566361266}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,360] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,361] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,363] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,363] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,364] Trial 16 finished with value: 0.5 and parameters: {'iterations': 674, 'learning_rate': 0.054403312289292724, 'depth': 7, 'l2_leaf_reg': 4.5890146903844675}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,375] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,376] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,377] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,378] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,379] Trial 17 finished with value: 0.5 and parameters: {'iterations': 559, 'learning_rate': 0.032770075817163305, 'depth': 6, 'l2_leaf_reg': 4.012550536973606}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,390] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,391] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,392] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,393] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,393] Trial 18 finished with value: 0.5 and parameters: {'iterations': 748, 'learning_rate': 0.09548424890644631, 'depth': 5, 'l2_leaf_reg': 2.888838705300288}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:15,405] 296 root - WARNING - Fold 1 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,406] 296 root - WARNING - Fold 2 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,408] 296 root - WARNING - Fold 3 failed: 'numpy.ndarray' object has no attribute 'iloc'\n",
            "[2025-07-15 21:09:15,408] 301 root - INFO - CatBoost Mean AUC: 0.5000\n",
            "[I 2025-07-15 21:09:15,409] Trial 19 finished with value: 0.5 and parameters: {'iterations': 865, 'learning_rate': 0.01046111763034609, 'depth': 8, 'l2_leaf_reg': 4.463401290631211}. Best is trial 0 with value: 0.5.\n",
            "[2025-07-15 21:09:23,512] 396 root - INFO - CatBoost_catboost_none_none | Test AUC: 0.9487\n",
            "[2025-07-15 21:09:23,513] 430 root - ERROR - Optimization failed for CatBoost_catboost_none_none: 'ColumnTransformer' object has no attribute 'named_transformers'\n",
            "[2025-07-15 21:09:23,515] 465 root - INFO - \n",
            "==================================================\n",
            "[2025-07-15 21:09:23,515] 466 root - INFO - BEST MODEL: RandomForest_onehot_minmax_mutual_info\n",
            "[2025-07-15 21:09:23,515] 467 root - INFO - ==================================================\n",
            "C:\\Users\\Nihal\\AppData\\Local\\Temp\\ipykernel_8768\\2820336604.py:484: FutureWarning:\n",
            "\n",
            "\n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "\n",
            "C:\\Users\\Nihal\\AppData\\Local\\Temp\\ipykernel_8768\\2820336604.py:522: FutureWarning:\n",
            "\n",
            "\n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "\n",
            "[2025-07-15 21:09:24,833] 601 root - INFO - \n",
            "Best Model Metrics:\n",
            "[2025-07-15 21:09:24,834] 604 root - INFO - accuracy: 0.9794\n",
            "[2025-07-15 21:09:24,834] 604 root - INFO - precision: 0.9813\n",
            "[2025-07-15 21:09:24,835] 604 root - INFO - recall: 0.9794\n",
            "[2025-07-15 21:09:24,835] 604 root - INFO - f1: 0.9799\n",
            "[2025-07-15 21:09:24,836] 604 root - INFO - auc_roc: 0.9975\n",
            "[2025-07-15 21:09:24,836] 606 root - INFO - \n",
            "Classification Report:\n",
            "[2025-07-15 21:09:24,837] 607 root - INFO -               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       374\n",
            "           1       0.83      0.99      0.90        91\n",
            "           2       0.99      0.98      0.98       944\n",
            "\n",
            "    accuracy                           0.98      1409\n",
            "   macro avg       0.94      0.98      0.96      1409\n",
            "weighted avg       0.98      0.98      0.98      1409\n",
            "\n",
            "[2025-07-15 21:09:24,837] 609 root - INFO - AutoML process completed\n"
          ]
        }
      ],
      "source": [
        "class AutoMLTrainer:\n",
        "    def __init__(self, X, y, test_size=0.2, random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize AutoML Trainer\n",
        "        \n",
        "        Args:\n",
        "            X: Features DataFrame\n",
        "            y: Target Series\n",
        "            test_size: Size of test split\n",
        "            random_state: Random seed\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "        \n",
        "        # Prepare data\n",
        "        self._prepare_data()\n",
        "        \n",
        "        # Initialize results storage\n",
        "        self.results = []\n",
        "        self.best_model = None\n",
        "        self.best_model_name = None\n",
        "        self.best_pipeline = None\n",
        "        self.feature_importances = None\n",
        "        \n",
        "        # Configure components\n",
        "        self._configure_components()\n",
        "        \n",
        "        logging.info(\"AutoML Trainer initialized successfully\")\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"Prepare and validate data\"\"\"\n",
        "        # Split data\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=self.test_size, \n",
        "            stratify=self.y, random_state=self.random_state\n",
        "        )\n",
        "        \n",
        "        # Identify feature types\n",
        "        self.numerical_cols = self.X.select_dtypes(include=np.number).columns.tolist()\n",
        "        self.categorical_cols = self.X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "        \n",
        "        # Validate and encode target\n",
        "        self._validate_target()\n",
        "        \n",
        "        # Validate features\n",
        "        self._validate_features()\n",
        "        \n",
        "        logging.info(f\"Data prepared. Train: {self.X_train.shape}, Test: {self.X_test.shape}\")\n",
        "        logging.info(f\"Numerical features: {len(self.numerical_cols)}, Categorical features: {len(self.categorical_cols)}\")\n",
        "\n",
        "    def _validate_target(self):\n",
        "        \"\"\"Validate and encode target variable\"\"\"\n",
        "        if self.y_train.dtype == 'object' or self.y_train.dtype.name == 'category':\n",
        "            self.label_encoder = LabelEncoder()\n",
        "            self.y_train = self.label_encoder.fit_transform(self.y_train)\n",
        "            self.y_test = self.label_encoder.transform(self.y_test)\n",
        "            logging.info(\"Encoded categorical target variable\")\n",
        "        \n",
        "        # Check classification type\n",
        "        self.n_classes = len(np.unique(self.y_train))\n",
        "        self.problem_type = \"binary\" if self.n_classes == 2 else \"multiclass\"\n",
        "        logging.info(f\"{self.problem_type.capitalize()} classification task detected\")\n",
        "\n",
        "    def _validate_features(self):\n",
        "        \"\"\"Validate features and remove problematic columns\"\"\"\n",
        "        # Remove constant features\n",
        "        constant_cols = []\n",
        "        for col in self.numerical_cols:\n",
        "            if self.X_train[col].nunique() == 1:\n",
        "                constant_cols.append(col)\n",
        "        \n",
        "        if constant_cols:\n",
        "            logging.warning(f\"Removing constant features: {constant_cols}\")\n",
        "            self.numerical_cols = [col for col in self.numerical_cols if col not in constant_cols]\n",
        "        \n",
        "        # Check for missing values\n",
        "        self._check_missing_values()\n",
        "\n",
        "    def _check_missing_values(self):\n",
        "        \"\"\"Check and log missing values\"\"\"\n",
        "        missing_num = self.X_train[self.numerical_cols].isna().sum().sum()\n",
        "        missing_cat = self.X_train[self.categorical_cols].isna().sum().sum()\n",
        "        \n",
        "        if missing_num or missing_cat:\n",
        "            logging.warning(f\"Missing values detected - Numerical: {missing_num}, Categorical: {missing_cat}\")\n",
        "\n",
        "    def _configure_components(self):\n",
        "        \"\"\"Configure encoders, scalers, and models\"\"\"\n",
        "        # Preprocessing components (remain as before for 'none' to skip steps)\n",
        "        self.encoders = {\n",
        "            'onehot': OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
        "            'target': TargetEncoder(),\n",
        "            'catboost': CatBoostEncoder()\n",
        "        }\n",
        "\n",
        "        self.scalers = {\n",
        "            'standard': StandardScaler(),\n",
        "            'minmax': MinMaxScaler(),\n",
        "            'none': None\n",
        "        }\n",
        "\n",
        "        self.feature_selectors = {\n",
        "            'variance': VarianceThreshold(threshold=0.01),\n",
        "            'mutual_info': SelectKBest(score_func=mutual_info_classif, k='all'),\n",
        "            'none': None\n",
        "        }\n",
        "\n",
        "        self.models = {\n",
        "            'LogisticRegression': {\n",
        "                'model': LogisticRegression,\n",
        "                'params': lambda trial: {\n",
        "                    'classifier__C': trial.suggest_float('C', 1e-3, 100, log=True),\n",
        "                    'classifier__solver': trial.suggest_categorical('solver', ['lbfgs', 'saga', 'liblinear']),\n",
        "                    'classifier__max_iter': 1000,\n",
        "                    'classifier__class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
        "                    'classifier__n_jobs': -1 if trial.suggest_categorical('lr_n_jobs', [True, False]) else 1\n",
        "                }\n",
        "            },\n",
        "            'RandomForest': {\n",
        "                'model': RandomForestClassifier,\n",
        "                'params': lambda trial: {\n",
        "                    'classifier__n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "                    'classifier__max_depth': trial.suggest_int('max_depth', 5, 20),\n",
        "                    'classifier__min_samples_split': trial.suggest_int('min_samples_split', 2, 15),\n",
        "                    'classifier__class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample', None]),\n",
        "                    'classifier__n_jobs': -1\n",
        "                }\n",
        "            },\n",
        "            'SVM': {\n",
        "                'model': SVC,\n",
        "                'params': lambda trial: {\n",
        "                    'classifier__C': trial.suggest_float('C', 1e-2, 100, log=True),\n",
        "                    'classifier__kernel': trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly']),\n",
        "                    'classifier__gamma': trial.suggest_categorical('gamma', ['scale', 'auto']) if trial.suggest_categorical('gamma_choice', [True, False]) else trial.suggest_float('gamma_custom', 1e-4, 1e-1, log=True),\n",
        "                    'classifier__degree': trial.suggest_int('degree', 2, 5) if trial.using_param('kernel', 'poly') else 3, # Only for 'poly' kernel\n",
        "                    'classifier__class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
        "                    'classifier__probability': True,\n",
        "                    'classifier__random_state': self.random_state\n",
        "                }\n",
        "            },\n",
        "            'XGBoost': {\n",
        "                'model': XGBClassifier,\n",
        "                'params': lambda trial: {\n",
        "                    'classifier__n_estimators': trial.suggest_int('n_estimators', 100, 1000), \n",
        "                    'classifier__learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True), \n",
        "                    'classifier__max_depth': trial.suggest_int('max_depth', 3, 10), \n",
        "                    'classifier__subsample': trial.suggest_float('subsample', 0.6, 1.0), \n",
        "                    'classifier__colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0), \n",
        "                    'classifier__reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True), \n",
        "                    'classifier__reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True), \n",
        "                    'classifier__gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True), \n",
        "                    'classifier__use_label_encoder': False, \n",
        "                    'classifier__eval_metric': 'logloss' if self.problem_type == \"binary\" else 'mlogloss', \n",
        "                    'classifier__objective': 'binary:logistic' if self.problem_type == \"binary\" else 'multi:softprob', \n",
        "                    'classifier__scale_pos_weight': trial.suggest_categorical('scale_pos_weight', [1, None]) if self.problem_type == \"binary\" else 1, \n",
        "                    'classifier__tree_method': 'hist',\n",
        "                    'classifier__random_state': self.random_state\n",
        "                }\n",
        "            },\n",
        "            'CatBoost': {\n",
        "                'model': CatBoostClassifier,\n",
        "                'params': lambda trial: {\n",
        "                    'classifier__iterations': trial.suggest_int('iterations', 500, 1500), # Increased range\n",
        "                    'classifier__learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "                    'classifier__depth': trial.suggest_int('depth', 4, 10), # Increased depth\n",
        "                    'classifier__l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10, log=True), # Broader L2 range\n",
        "                    'classifier__min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 30), # Added\n",
        "                    'classifier__auto_class_weights': trial.suggest_categorical('auto_class_weights', ['Balanced', 'SqrtBalanced', False]),\n",
        "                    'classifier__loss_function': 'Logloss' if self.problem_type == \"binary\" else 'MultiClass', # Specify loss\n",
        "                    'classifier__eval_metric': 'AUC' if self.problem_type == \"binary\" else 'MultiClassOneVsAll', # Specify eval metric\n",
        "                    'classifier__verbose': False,\n",
        "                    'classifier__task_type': 'CPU',\n",
        "                    'classifier__random_seed': self.random_state\n",
        "                }\n",
        "            },\n",
        "            'LightGBM': {\n",
        "                'model': LGBMClassifier,\n",
        "                'params': lambda trial: {\n",
        "                    'classifier__n_estimators': trial.suggest_int('n_estimators', 100, 1000), # Increased range\n",
        "                    'classifier__learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "                    'classifier__num_leaves': trial.suggest_int('num_leaves', 20, 200), # Increased range\n",
        "                    'classifier__max_depth': trial.suggest_int('max_depth', 3, 12), # Increased depth\n",
        "                    'classifier__min_child_samples': trial.suggest_int('min_child_samples', 10, 80), # Broader range\n",
        "                    'classifier__subsample': trial.suggest_float('subsample', 0.6, 1.0), # Added\n",
        "                    'classifier__colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0), # Added\n",
        "                    'classifier__reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True), # L1 regularization\n",
        "                    'classifier__reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True), # L2 regularization\n",
        "                    'classifier__class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
        "                    'classifier__objective': 'binary' if self.problem_type == \"binary\" else 'multiclass', # Specify objective\n",
        "                    'classifier__metric': 'auc' if self.problem_type == \"binary\" else 'multi_logloss', # Specify metric\n",
        "                    'classifier__n_jobs': -1,\n",
        "                    'classifier__random_state': self.random_state\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        logging.info(\"Components configured: %d encoders, %d scalers, %d feature selectors, %d models\",\n",
        "                     len(self.encoders), len(self.scalers), len(self.feature_selectors), len(self.models))\n",
        "        \n",
        "    def create_pipeline(self, encoder_name, scaler_name, selector_name, model_name):\n",
        "        \"\"\"\n",
        "        Create robust pipeline with feature selection\n",
        "        \n",
        "        Args:\n",
        "            encoder_name: Name of categorical encoder\n",
        "            scaler_name: Name of numerical scaler\n",
        "            selector_name: Name of feature selector\n",
        "            model_name: Name of model\n",
        "            \n",
        "        Returns:\n",
        "            Configured pipeline\n",
        "        \"\"\"\n",
        "        # Numerical pipeline\n",
        "        num_steps = [('imputer', SimpleImputer(strategy='median'))]\n",
        "        \n",
        "        if scaler_name != 'none':\n",
        "            num_steps.append(('scaler', self.scalers[scaler_name]))\n",
        "            \n",
        "        num_pipeline = Pipeline(num_steps)\n",
        "        \n",
        "        # Categorical pipeline\n",
        "        cat_pipeline = Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', self.encoders[encoder_name])\n",
        "        ])\n",
        "        \n",
        "        # Preprocessor\n",
        "        preprocessor = ColumnTransformer([\n",
        "            ('num', num_pipeline, self.numerical_cols),\n",
        "            ('cat', cat_pipeline, self.categorical_cols)\n",
        "        ])\n",
        "        \n",
        "        # Main pipeline\n",
        "        pipeline_steps = [('preprocessor', preprocessor)]\n",
        "        \n",
        "        # Feature selection\n",
        "        if selector_name != 'none':\n",
        "            pipeline_steps.append(('feature_selector', self.feature_selectors[selector_name]))\n",
        "        \n",
        "        # Model\n",
        "        pipeline_steps.append(('classifier', self.models[model_name]['model']()))\n",
        "        \n",
        "        return Pipeline(pipeline_steps)\n",
        "\n",
        "    def objective(self, trial, encoder_name, scaler_name, selector_name, model_name):\n",
        "        \"\"\"\n",
        "        Objective function for Optuna optimization\n",
        "        \n",
        "        Args:\n",
        "            trial: Optuna trial object\n",
        "            encoder_name: Encoder to use\n",
        "            scaler_name: Scaler to use\n",
        "            selector_name: Feature selector to use\n",
        "            model_name: Model to use\n",
        "            \n",
        "        Returns:\n",
        "            Mean CV score\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Create pipeline\n",
        "            pipeline = self.create_pipeline(encoder_name, scaler_name, selector_name, model_name)\n",
        "            \n",
        "            # Set hyperparameters\n",
        "            params = self.models[model_name]['params'](trial)\n",
        "            pipeline.set_params(**params)\n",
        "            \n",
        "            # Manual cross-validation with error handling\n",
        "            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)\n",
        "            scores = []\n",
        "            \n",
        "            for fold, (train_idx, val_idx) in enumerate(cv.split(self.X_train, self.y_train)):\n",
        "                try:\n",
        "                    # Train on fold\n",
        "                    X_fold_train = self.X_train.iloc[train_idx]\n",
        "                    y_fold_train = self.y_train.iloc[train_idx]\n",
        "                    pipeline.fit(X_fold_train, y_fold_train)\n",
        "                    \n",
        "                    # Validate on fold\n",
        "                    X_val = self.X_train.iloc[val_idx]\n",
        "                    y_val = self.y_train.iloc[val_idx]\n",
        "                    \n",
        "                    # Calculate score\n",
        "                    if hasattr(pipeline.named_steps['classifier'], 'predict_proba'):\n",
        "                        y_proba = pipeline.predict_proba(X_val)\n",
        "                        score = roc_auc_score(y_val, y_proba[:, 1] if self.problem_type == \"binary\" else y_proba, \n",
        "                                              multi_class='ovr' if self.problem_type == \"multiclass\" else 'raise')\n",
        "                    else:\n",
        "                        y_pred = pipeline.predict(X_val)\n",
        "                        score = roc_auc_score(y_val, y_pred)\n",
        "                    \n",
        "                    scores.append(score)\n",
        "                    logging.debug(f\"Fold {fold+1}: {model_name} AUC = {score:.4f}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Fold {fold+1} failed: {str(e)}\")\n",
        "                    scores.append(0.5)  # Neutral score for failed fold\n",
        "            \n",
        "            # Calculate mean score\n",
        "            mean_score = np.mean(scores)\n",
        "            logging.info(f\"{model_name} Mean AUC: {mean_score:.4f}\")\n",
        "            return mean_score\n",
        "        \n",
        "        except Exception as e:\n",
        "            logging.error(f\"Objective failed: {str(e)}\")\n",
        "            return float('-inf')\n",
        "\n",
        "    def optimize(self, n_trials=20, timeout=600):\n",
        "        \"\"\"\n",
        "        Optimize model with feature selection\n",
        "        \n",
        "        Args:\n",
        "            n_trials: Number of trials per configuration\n",
        "            timeout: Timeout per configuration in seconds\n",
        "        \"\"\"\n",
        "        self.results = []\n",
        "        \n",
        "        # Test a subset of configurations for demonstration\n",
        "        configurations = [\n",
        "            ('target', 'standard', 'variance', 'XGBoost'),\n",
        "            ('onehot', 'minmax', 'mutual_info', 'RandomForest'),\n",
        "            ('catboost', 'none', 'none', 'CatBoost')\n",
        "        ]\n",
        "        \n",
        "        self.results = []\n",
        "        \n",
        "        # Dynamically generate all possible configurations\n",
        "        all_encoders = self.encoders.keys()\n",
        "        all_scalers = self.scalers.keys()\n",
        "        all_selectors = self.feature_selectors.keys()\n",
        "        all_models = self.models.keys()\n",
        "\n",
        "        configurations = []\n",
        "        for encoder_name in all_encoders:\n",
        "            for scaler_name in all_scalers:\n",
        "                for selector_name in all_selectors:\n",
        "                    for model_name in all_models:\n",
        "                        configurations.append((encoder_name, scaler_name, selector_name, model_name))\n",
        "\n",
        "        # IMPORTANT: This will create a very large number of configurations!\n",
        "        # Consider a smaller subset for initial testing, or implement a more\n",
        "        # sophisticated search strategy (e.g., genetic algorithms, multi-objective optimization)\n",
        "        # to select configurations if the search space becomes too vast.\n",
        "        # For demonstration purposes, let's limit it to a reasonable number if needed\n",
        "        # Or, you could remove the hardcoded list altogether.\n",
        "        # For this example, let's just use the dynamically generated list as is.\n",
        "        \n",
        "        logging.info(f\"Total configurations to evaluate: {len(configurations)}\")\n",
        "\n",
        "        for config in configurations:\n",
        "            encoder_name, scaler_name, selector_name, model_name = config\n",
        "            config_name = f\"{model_name}_{encoder_name}_{scaler_name}_{selector_name}\"\n",
        "            \n",
        "            logging.info(f\"\\n{'='*50}\")\n",
        "            logging.info(f\"Optimizing {config_name}\")\n",
        "            logging.info(f\"{'='*50}\")\n",
        "            \n",
        "            try:\n",
        "                # Create study\n",
        "                study = optuna.create_study(\n",
        "                    direction='maximize',\n",
        "                    sampler=optuna.samplers.TPESampler(seed=self.random_state),\n",
        "                    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
        "                )\n",
        "                \n",
        "                # Optimize\n",
        "                study.optimize(\n",
        "                    lambda trial: self.objective(trial, encoder_name, scaler_name, selector_name, model_name),\n",
        "                    n_trials=n_trials,\n",
        "                    timeout=timeout\n",
        "                )\n",
        "                \n",
        "                # ... (rest of your existing optimize method code) ...\n",
        "                \n",
        "                # Skip if no successful trials\n",
        "                if not study.best_trial:\n",
        "                    logging.warning(f\"No successful trials for {config_name}\")\n",
        "                    continue\n",
        "                \n",
        "                # Train best pipeline\n",
        "                best_pipeline = self.create_pipeline(encoder_name, scaler_name, selector_name, model_name)\n",
        "                best_params = self.models[model_name]['params'](study.best_trial)\n",
        "                best_pipeline.set_params(**best_params)\n",
        "                best_pipeline.fit(self.X_train, self.y_train)\n",
        "                \n",
        "                # Evaluate\n",
        "                y_pred = best_pipeline.predict(self.X_test)\n",
        "                # Corrected: Handle multiclass predict_proba for AUC\n",
        "                if hasattr(best_pipeline.named_steps['classifier'], 'predict_proba'):\n",
        "                    y_proba = best_pipeline.predict_proba(self.X_test)\n",
        "                    if self.problem_type == \"binary\":\n",
        "                        y_proba_score = y_proba[:, 1]\n",
        "                    else: # Multiclass\n",
        "                        y_proba_score = y_proba\n",
        "                else:\n",
        "                    y_proba_score = None\n",
        "                \n",
        "                # Calculate metrics\n",
        "                metrics = {\n",
        "                    'config': config_name,\n",
        "                    'model': model_name,\n",
        "                    'encoder': encoder_name,\n",
        "                    'scaler': scaler_name,\n",
        "                    'selector': selector_name,\n",
        "                    'accuracy': accuracy_score(self.y_test, y_pred),\n",
        "                    'precision': precision_score(self.y_test, y_pred, average='weighted'),\n",
        "                    'recall': recall_score(self.y_test, y_pred, average='weighted'),\n",
        "                    'f1': f1_score(self.y_test, y_pred, average='weighted'),\n",
        "                    'best_params': best_params,\n",
        "                    'best_pipeline_obj': best_pipeline # Store the trained pipeline object\n",
        "                }\n",
        "                \n",
        "                # Add AUC if available\n",
        "                if y_proba_score is not None:\n",
        "                    if self.problem_type == \"binary\":\n",
        "                        metrics['auc_roc'] = roc_auc_score(self.y_test, y_proba_score)\n",
        "                    else: # Multiclass\n",
        "                        metrics['auc_roc'] = roc_auc_score(self.y_test, y_proba_score, multi_class='ovr')\n",
        "                elif self.problem_type == \"binary\":\n",
        "                    metrics['auc_roc'] = roc_auc_score(self.y_test, y_pred) # Fallback for binary if no predict_proba\n",
        "                \n",
        "                self.results.append(metrics)\n",
        "                logging.info(f\"{config_name} | Test AUC: {metrics.get('auc_roc', np.nan):.4f}\")\n",
        "                \n",
        "                # Store feature importances\n",
        "                if hasattr(best_pipeline.named_steps['classifier'], 'feature_importances_'):\n",
        "                    # Accessing feature names correctly after preprocessing\n",
        "                    # This part can be tricky as ColumnTransformer changes column names\n",
        "                    # For simplicity, we'll store them for the best model after fitting\n",
        "                    self.feature_importances_for_plotting = best_pipeline.named_steps['classifier'].feature_importances_\n",
        "                    \n",
        "                    # Try to get feature names after preprocessing for more accurate plotting\n",
        "                    # This is a common challenge with pipelines; for simplicity, we'll use original names + OHE for now\n",
        "                    preprocessor_fitted = best_pipeline.named_steps['preprocessor']\n",
        "                    \n",
        "                    # Get feature names after one-hot encoding if applicable\n",
        "                    if 'cat' in preprocessor_fitted.named_transformers and hasattr(preprocessor_fitted.named_transformers['cat'].named_steps['encoder'], 'get_feature_names_out'):\n",
        "                         cat_feature_names = preprocessor_fitted.named_transformers['cat'].named_steps['encoder'].get_feature_names_out(self.categorical_cols)\n",
        "                    else:\n",
        "                         cat_feature_names = self.categorical_cols\n",
        "                    \n",
        "                    processed_feature_names = self.numerical_cols + list(cat_feature_names) # Combine original numerical and new categorical\n",
        "                    \n",
        "                    # If feature selection is applied, you'll need to map importances to selected features\n",
        "                    if selector_name != 'none':\n",
        "                        selector = best_pipeline.named_steps['feature_selector']\n",
        "                        if hasattr(selector, 'get_support'):\n",
        "                            selected_indices = selector.get_support(indices=True)\n",
        "                            processed_feature_names = [processed_feature_names[i] for i in selected_indices]\n",
        "                            \n",
        "                    self.feature_names_for_plotting = processed_feature_names\n",
        "\n",
        "                # Visualize optimization\n",
        "                self._plot_optimization(study, config_name)\n",
        "                \n",
        "            except Exception as e:\n",
        "                logging.error(f\"Optimization failed for {config_name}: {str(e)}\")\n",
        "        \n",
        "        # Finalize results\n",
        "        if self.results:\n",
        "            self._finalize_results()\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _plot_optimization(self, study, config_name):\n",
        "        \"\"\"Visualize optimization results\"\"\"\n",
        "        try:\n",
        "            # Optimization history\n",
        "            fig = plot_optimization_history(study)\n",
        "            fig.update_layout(title=f\"{config_name} Optimization History\")\n",
        "            fig.write_image(f\"opt_history_{config_name}.png\")\n",
        "            \n",
        "            # Parameter importance\n",
        "            fig = plot_param_importances(study)\n",
        "            fig.update_layout(title=f\"{config_name} Parameter Importance\")\n",
        "            fig.write_image(f\"param_importance_{config_name}.png\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logging.error(f\"Visualization failed: {str(e)}\")\n",
        "\n",
        "    def _finalize_results(self):\n",
        "        \"\"\"Process and store final results\"\"\"\n",
        "        self.results_df = pd.DataFrame(self.results)\n",
        "        \n",
        "        # Sort and select best model\n",
        "        self.results_df = self.results_df.sort_values('auc_roc', ascending=False)\n",
        "        best_row = self.results_df.iloc[0]\n",
        "        self.best_model_name = best_row['config']\n",
        "        # Corrected: Access the stored best_pipeline_obj\n",
        "        self.best_pipeline = best_row['best_pipeline_obj'] \n",
        "        \n",
        "        logging.info(f\"\\n{'='*50}\")\n",
        "        logging.info(f\"BEST MODEL: {self.best_model_name}\")\n",
        "        logging.info(f\"{'='*50}\")\n",
        "        \n",
        "        # Save results\n",
        "        self.results_df.drop(columns=['best_pipeline_obj'], inplace=True) # Drop the pipeline object before saving CSV\n",
        "        self.results_df.to_csv('automl_results.csv', index=False)\n",
        "        joblib.dump(self.best_pipeline, 'best_model.pkl')\n",
        "        \n",
        "        # Plot model comparison\n",
        "        self.plot_model_comparison()\n",
        "        \n",
        "        # Plot feature importances if available\n",
        "        if hasattr(self, 'feature_importances_for_plotting') and self.feature_importances_for_plotting is not None:\n",
        "            self.plot_feature_importances()\n",
        "\n",
        "    def plot_model_comparison(self):\n",
        "        \"\"\"Visualize model comparison\"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(\n",
        "            x='auc_roc', \n",
        "            y='config', \n",
        "            data=self.results_df.sort_values('auc_roc', ascending=False),\n",
        "            palette='viridis'\n",
        "        )\n",
        "        plt.title('Model Performance Comparison (AUC-ROC)', fontsize=16)\n",
        "        plt.xlabel('AUC-ROC Score', fontsize=14)\n",
        "        plt.ylabel('Configuration', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('model_performance_comparison.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    def plot_feature_importances(self, top_n=20):\n",
        "        \"\"\"Plot top feature importances\"\"\"\n",
        "        if not hasattr(self, 'feature_importances_for_plotting') or self.feature_importances_for_plotting is None:\n",
        "            logging.warning(\"Feature importances not available for plotting\")\n",
        "            return\n",
        "        \n",
        "        # Get feature names - using the stored names after preprocessing if available\n",
        "        if hasattr(self, 'feature_names_for_plotting') and self.feature_names_for_plotting:\n",
        "            feature_names = self.feature_names_for_plotting\n",
        "        else:\n",
        "            feature_names = self.numerical_cols + self.categorical_cols # Fallback\n",
        "            \n",
        "        # Ensure feature_importances_for_plotting and feature_names have the same length\n",
        "        if len(feature_names) != len(self.feature_importances_for_plotting):\n",
        "            logging.warning(f\"Mismatch in feature importance array length ({len(self.feature_importances_for_plotting)}) and feature names length ({len(feature_names)}). Cannot plot feature importances correctly.\")\n",
        "            return\n",
        "\n",
        "        # Create importance DataFrame\n",
        "        importances = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': self.feature_importances_for_plotting\n",
        "        }).sort_values('importance', ascending=False).head(top_n)\n",
        "        \n",
        "        # Plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='importance', y='feature', data=importances, palette='rocket')\n",
        "        plt.title('Top Feature Importances', fontsize=16)\n",
        "        plt.xlabel('Importance', fontsize=14)\n",
        "        plt.ylabel('Feature', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('feature_importances.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Comprehensive evaluation of best model\"\"\"\n",
        "        if not self.best_pipeline:\n",
        "            logging.error(\"No best model available for evaluation\")\n",
        "            return None\n",
        "        \n",
        "        # Predictions\n",
        "        y_pred = self.best_pipeline.predict(self.X_test)\n",
        "        \n",
        "        # Corrected: Handle multiclass predict_proba for AUC in evaluation\n",
        "        if hasattr(self.best_pipeline.named_steps['classifier'], 'predict_proba'):\n",
        "            y_proba = self.best_pipeline.predict_proba(self.X_test)\n",
        "            if self.problem_type == \"binary\":\n",
        "                y_proba_score = y_proba[:, 1]\n",
        "            else: # Multiclass\n",
        "                y_proba_score = y_proba\n",
        "        else:\n",
        "            y_proba_score = None\n",
        "            \n",
        "        # Metrics\n",
        "        metrics = {\n",
        "            \"accuracy\": accuracy_score(self.y_test, y_pred),\n",
        "            \"precision\": precision_score(self.y_test, y_pred, average='weighted'),\n",
        "            \"recall\": recall_score(self.y_test, y_pred, average='weighted'),\n",
        "            \"f1\": f1_score(self.y_test, y_pred, average='weighted'),\n",
        "            \"classification_report\": classification_report(self.y_test, y_pred)\n",
        "        }\n",
        "        \n",
        "        # AUC if available\n",
        "        if y_proba_score is not None:\n",
        "            if self.problem_type == \"binary\":\n",
        "                metrics[\"auc_roc\"] = roc_auc_score(self.y_test, y_proba_score)\n",
        "            else: # Multiclass\n",
        "                metrics[\"auc_roc\"] = roc_auc_score(self.y_test, y_proba_score, multi_class='ovr')\n",
        "        \n",
        "        # Confusion matrix\n",
        "        self._plot_confusion_matrix(y_pred)\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "    def _plot_confusion_matrix(self, y_pred):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        cm = confusion_matrix(self.y_test, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                    xticklabels=['Predicted 0', 'Predicted 1'], # Consider dynamic labels for multiclass\n",
        "                    yticklabels=['Actual 0', 'Actual 1']) # Consider dynamic labels for multiclass\n",
        "        plt.title(f'Confusion Matrix: {self.best_model_name}', fontsize=14)\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'confusion_matrix_{self.best_model_name}.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    df = pd.read_csv(\"E:/VS_Code/Projects/Customer_Churn/notebooks/data/processed_customer_churn_data.csv\")\n",
        "    \n",
        "    # Initialize and run AutoML\n",
        "    automl = AutoMLTrainer(\n",
        "        X=df.drop(columns=['customer_status']),\n",
        "        y=df['customer_status'],\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    # Run optimization\n",
        "    if automl.optimize(n_trials=20, timeout=600):\n",
        "        # Evaluate best model\n",
        "        metrics = automl.evaluate()\n",
        "        logging.info(\"\\nBest Model Metrics:\")\n",
        "        for k, v in metrics.items():\n",
        "            if k != \"classification_report\":\n",
        "                logging.info(f\"{k}: {v:.4f}\")\n",
        "        \n",
        "        logging.info(\"\\nClassification Report:\")\n",
        "        logging.info(metrics[\"classification_report\"])\n",
        "    \n",
        "    logging.info(\"AutoML process completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>config</th>\n",
              "      <th>model</th>\n",
              "      <th>encoder</th>\n",
              "      <th>scaler</th>\n",
              "      <th>selector</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>best_params</th>\n",
              "      <th>auc_roc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RandomForest_onehot_minmax_mutual_info</td>\n",
              "      <td>RandomForest</td>\n",
              "      <td>onehot</td>\n",
              "      <td>minmax</td>\n",
              "      <td>mutual_info</td>\n",
              "      <td>0.979418</td>\n",
              "      <td>0.981345</td>\n",
              "      <td>0.979418</td>\n",
              "      <td>0.979906</td>\n",
              "      <td>{'classifier__n_estimators': 250, 'classifier_...</td>\n",
              "      <td>0.997513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CatBoost_catboost_none_none</td>\n",
              "      <td>CatBoost</td>\n",
              "      <td>catboost</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>0.838893</td>\n",
              "      <td>0.844584</td>\n",
              "      <td>0.838893</td>\n",
              "      <td>0.840470</td>\n",
              "      <td>{'classifier__iterations': 687, 'classifier__l...</td>\n",
              "      <td>0.948663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost_target_standard_variance</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>target</td>\n",
              "      <td>standard</td>\n",
              "      <td>variance</td>\n",
              "      <td>0.734564</td>\n",
              "      <td>0.557034</td>\n",
              "      <td>0.734564</td>\n",
              "      <td>0.630050</td>\n",
              "      <td>{'classifier__n_estimators': 250, 'classifier_...</td>\n",
              "      <td>0.793307</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   config         model   encoder    scaler  \\\n",
              "0  RandomForest_onehot_minmax_mutual_info  RandomForest    onehot    minmax   \n",
              "1             CatBoost_catboost_none_none      CatBoost  catboost      none   \n",
              "2        XGBoost_target_standard_variance       XGBoost    target  standard   \n",
              "\n",
              "      selector  accuracy  precision    recall        f1  \\\n",
              "0  mutual_info  0.979418   0.981345  0.979418  0.979906   \n",
              "1         none  0.838893   0.844584  0.838893  0.840470   \n",
              "2     variance  0.734564   0.557034  0.734564  0.630050   \n",
              "\n",
              "                                         best_params   auc_roc  \n",
              "0  {'classifier__n_estimators': 250, 'classifier_...  0.997513  \n",
              "1  {'classifier__iterations': 687, 'classifier__l...  0.948663  \n",
              "2  {'classifier__n_estimators': 250, 'classifier_...  0.793307  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_model_results_df = pd.read_csv('automl_results.csv')\n",
        "all_model_results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>config</th>\n",
              "      <th>model</th>\n",
              "      <th>encoder</th>\n",
              "      <th>scaler</th>\n",
              "      <th>selector</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>best_params</th>\n",
              "      <th>auc_roc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RandomForest_onehot_minmax_mutual_info</td>\n",
              "      <td>RandomForest</td>\n",
              "      <td>onehot</td>\n",
              "      <td>minmax</td>\n",
              "      <td>mutual_info</td>\n",
              "      <td>0.979418</td>\n",
              "      <td>0.981345</td>\n",
              "      <td>0.979418</td>\n",
              "      <td>0.979906</td>\n",
              "      <td>{'classifier__n_estimators': 250, 'classifier_...</td>\n",
              "      <td>0.997513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CatBoost_catboost_none_none</td>\n",
              "      <td>CatBoost</td>\n",
              "      <td>catboost</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>0.838893</td>\n",
              "      <td>0.844584</td>\n",
              "      <td>0.838893</td>\n",
              "      <td>0.840470</td>\n",
              "      <td>{'classifier__iterations': 687, 'classifier__l...</td>\n",
              "      <td>0.948663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost_target_standard_variance</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>target</td>\n",
              "      <td>standard</td>\n",
              "      <td>variance</td>\n",
              "      <td>0.734564</td>\n",
              "      <td>0.557034</td>\n",
              "      <td>0.734564</td>\n",
              "      <td>0.630050</td>\n",
              "      <td>{'classifier__n_estimators': 250, 'classifier_...</td>\n",
              "      <td>0.793307</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   config         model   encoder    scaler  \\\n",
              "0  RandomForest_onehot_minmax_mutual_info  RandomForest    onehot    minmax   \n",
              "1             CatBoost_catboost_none_none      CatBoost  catboost      none   \n",
              "2        XGBoost_target_standard_variance       XGBoost    target  standard   \n",
              "\n",
              "      selector  accuracy  precision    recall        f1  \\\n",
              "0  mutual_info  0.979418   0.981345  0.979418  0.979906   \n",
              "1         none  0.838893   0.844584  0.838893  0.840470   \n",
              "2     variance  0.734564   0.557034  0.734564  0.630050   \n",
              "\n",
              "                                         best_params   auc_roc  \n",
              "0  {'classifier__n_estimators': 250, 'classifier_...  0.997513  \n",
              "1  {'classifier__iterations': 687, 'classifier__l...  0.948663  \n",
              "2  {'classifier__n_estimators': 250, 'classifier_...  0.793307  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Top 5 models by AUC-ROC\n",
        "all_model_results_df.sort_values(by='auc_roc', ascending=False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>config</th>\n",
              "      <th>model</th>\n",
              "      <th>encoder</th>\n",
              "      <th>scaler</th>\n",
              "      <th>selector</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>best_params</th>\n",
              "      <th>auc_roc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost_target_standard_variance</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>target</td>\n",
              "      <td>standard</td>\n",
              "      <td>variance</td>\n",
              "      <td>0.734564</td>\n",
              "      <td>0.557034</td>\n",
              "      <td>0.734564</td>\n",
              "      <td>0.63005</td>\n",
              "      <td>{'classifier__n_estimators': 250, 'classifier_...</td>\n",
              "      <td>0.793307</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             config    model encoder    scaler  selector  \\\n",
              "2  XGBoost_target_standard_variance  XGBoost  target  standard  variance   \n",
              "\n",
              "   accuracy  precision    recall       f1  \\\n",
              "2  0.734564   0.557034  0.734564  0.63005   \n",
              "\n",
              "                                         best_params   auc_roc  \n",
              "2  {'classifier__n_estimators': 250, 'classifier_...  0.793307  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Results for XGBoost models\n",
        "all_model_results_df[all_model_results_df['model'] == 'XGBoost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'AutoMLTrainer' object has no attribute 'feature_names_for_plotting'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(automl, \u001b[33m'\u001b[39m\u001b[33mfeature_importances_for_plotting\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m automl.feature_importances_for_plotting \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      2\u001b[39m     feature_importances = automl.feature_importances_for_plotting\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     feature_names = \u001b[43mautoml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_names_for_plotting\u001b[49m \u001b[38;5;66;03m# This will be the processed names\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feature_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feature_names) == \u001b[38;5;28mlen\u001b[39m(feature_importances):\n\u001b[32m      6\u001b[39m         importance_df = pd.DataFrame({\n\u001b[32m      7\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m'\u001b[39m: feature_names,\n\u001b[32m      8\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m: feature_importances\n\u001b[32m      9\u001b[39m         }).sort_values(\u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'AutoMLTrainer' object has no attribute 'feature_names_for_plotting'"
          ]
        }
      ],
      "source": [
        "if hasattr(automl, 'feature_importances_for_plotting') and automl.feature_importances_for_plotting is not None:\n",
        "    feature_importances = automl.feature_importances_for_plotting\n",
        "    feature_names = automl.feature_names_for_plotting # This will be the processed names\n",
        "\n",
        "    if feature_names and len(feature_names) == len(feature_importances):\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': feature_importances\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        print(\"\\nRaw Feature Importances DataFrame:\")\n",
        "        print(importance_df.head(20)) # Print top 20 for example\n",
        "\n",
        "        # You can also regenerate the plot with custom settings if needed\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='importance', y='feature', data=importance_df.head(20), palette='viridis')\n",
        "        plt.title('Custom Top Feature Importances Plot', fontsize=16)\n",
        "        plt.xlabel('Importance', fontsize=14)\n",
        "        plt.ylabel('Feature', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Feature names or importances are not correctly available for custom plotting.\")\n",
        "else:\n",
        "    print(\"Feature importances were not calculated or stored for the best model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9907735982966643\n"
          ]
        }
      ],
      "source": [
        "pipe = trainer.create_pipeline('onehot', 'LogisticRegression')\n",
        "pipe.set_params(**{\n",
        "    'classifier__C': 1.0,\n",
        "    'classifier__solver': 'lbfgs'\n",
        "})\n",
        "pipe.fit(trainer.x_train, trainer.y_train)\n",
        "print(pipe.score(trainer.x_test, trainer.y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7V5lAKNRr7xG",
        "w9XR_PT8jpa5",
        "zmP91_c2od3X"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
